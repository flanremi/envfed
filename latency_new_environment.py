# getPCA25()
import gc
import json
import math
import random
import time

from enum import Enum

import numpy as np
import torch
import torch.nn as nn

import os
import torch
import argparse
from tqdm import tqdm

from torchsummary import summary
from pca_helper import get_pca_by_model

from train_helper import TrainingHelper, Opt

constant_add = [[-0.013950000000000002, -0.0398, 0.049850000000000005, 0.01755, 0.04680000000000001, -0.0245, 0.0476,
                 -0.031900000000000005, 0.00445, 0.0391],
                [0.0014000000000000002, 0.03725, 0.03395, 0.042300000000000004, -0.0148, -0.02155, 0.03185, 0.049300000000000004, -0.02475, 0.0033000000000000004],
                [-0.019000000000000003, -0.046000000000000006, -0.0183, -0.02995, -0.02595, -0.0047, -0.022150000000000003, 0.025550000000000003, 0.03245, -0.0396],
                [0.031650000000000005, -0.0476, 0.007000000000000001, 0.0305, -0.0166, 0.02435, -0.032600000000000004, -0.0155, 0.013700000000000002, -0.00055],
                [-0.0072499999999999995, 0.03015, 0.01255, -0.022400000000000003, 0.025150000000000002, -0.0408, -0.0303, -0.0235, -0.0034000000000000002, 0.0164],
                [-0.0358, -0.006350000000000001, 0.0238, 0.03225, -0.039450000000000006, 0.009850000000000001, 0.04635, 0.0213, -0.03205, -0.0159], [0.01665, -0.04365, 0.020300000000000002, -0.00215, 0.046250000000000006, 0.042050000000000004, -0.03485, -0.0424, 0.03345, -0.04715], [-0.008350000000000002, -0.0376, -0.026600000000000002, -0.012650000000000002, -0.0347, -0.04025000000000001, -0.0357, -0.01775, -0.0228, -0.0216], [0.0332, -0.03205, -0.01445, -0.0374, 0.0424, -0.0079, 0.0337, 0.04585, 0.04285, 0.016200000000000003], [0.0415, 0.0237, -0.02885, 0.04285, 0.037, -0.0408, -0.027850000000000003, -0.027100000000000003, -0.0182, -0.031900000000000005], [-0.00865, 0.0246, -0.0299, 0.03525, -0.0207, -0.012400000000000001, 0.04975, 0.0016, -0.011600000000000001, 0.049800000000000004], [0.012700000000000001, 0.039400000000000004, 0.006350000000000001, -0.021400000000000002, -0.027150000000000004, 0.016200000000000003, 0.022150000000000003, -0.04725, 0.0059, 0.0221], [0.01085, 0.0386, -0.0002, -0.011250000000000001, -0.0483, 0.048850000000000005, -0.043750000000000004, -0.0261, 0.018, -0.02355], [0.008400000000000001, -0.02445, 0.01225, 0.0185, -0.03515, 0.03, -0.01635, -0.0031000000000000003, -0.0024500000000000004, 0.03575], [-0.014000000000000002, -0.009550000000000001, 0.0002, 0.0035000000000000005, -0.024300000000000002, -0.038200000000000005, 0.02535, 0.01615, 0.04365, -0.013650000000000002], [0.0004, 0.037700000000000004, -0.005000000000000001, 0.00615, -0.00755, -0.010950000000000001, 0.013300000000000001, 0.0349, 0.0222, 0.0213], [-0.013550000000000001, -0.04515, 0.0439, -0.02495, 0.04530000000000001, 0.0297, -0.02945, -0.005050000000000001, -0.0228, -0.027550000000000005], [-0.019600000000000003, 0.0046, 0.023350000000000003, 0.0218, 0.0194, -0.04655000000000001, -0.0009, -0.02865, -0.02075, 0.027200000000000002], [0.0439, 0.0316, -0.0475, 0.019850000000000003, 0.00065, 0.027700000000000002, 0.00215, 0.008100000000000001, -0.04945, -0.0216], [0.037500000000000006, 0.0023, 0.0015500000000000002, -0.00885, -0.0105, 0.009, -0.044050000000000006, 0.0228, 0.0006000000000000001, 0.01745], [0.0052, 0.013800000000000002, 0.00265, -0.040100000000000004, -0.023150000000000004, 0.0044, -0.03635, 0.022750000000000003, -0.0436, -0.00915], [0.01795, -0.04115, -0.04945, -0.00735, -0.026250000000000002, -0.03295, 0.0328, -0.015, 0.027350000000000003, 0.03505], [0.02015, -0.0346, -0.046700000000000005, 0.027900000000000005, -0.01515, 0.0091, -0.0396, 0.0296, 0.0011, -0.019700000000000002], [0.0262, 0.0159, 0.025800000000000003, -0.006900000000000001, -0.0439, 0.043000000000000003, 0.022250000000000002, -0.02915, 0.0396, -0.04735], [0.028000000000000004, -0.031400000000000004, 0.04085, -0.013950000000000002, -0.0318, -0.0495, 0.034999999999999996, 0.04105, 0.043750000000000004, 0.02245], [-0.006700000000000001, -0.0109, 0.047, 0.004200000000000001, 0.0121, 0.033800000000000004, -0.02595, -0.04020000000000001, 0.02895, -0.039900000000000005], [-0.008450000000000001, 0.03415000000000001, 0.0364, 0.0429, -0.043550000000000005, -0.003, 0.0041, -0.015600000000000001, -0.013450000000000002, 0.031100000000000003], [-0.04515, 0.0028000000000000004, 0.008150000000000001, -0.026400000000000003, -0.03625, 0.0495, 0.0483, 0.0349, 0.0122, -0.020250000000000004], [-0.04695, -0.0383, 0.0245, -0.0227, 0.0016, 0.00745, -0.023400000000000004, 0.017150000000000002, -0.028499999999999998, -0.04855], [-0.0193, -0.039700000000000006, -0.018850000000000002, 0.03605, -0.03835, -0.04775, 0.048850000000000005, -0.0239, 0.022600000000000002, -0.0396], [0.038400000000000004, 0.01575, 0.0459, 0.0488, 0.04605000000000001, -0.002, -0.03835, 0.04865, 0.028149999999999998, 0.022250000000000002], [0.0325, -0.0424, -0.011450000000000002, 0.0001, 0.021500000000000002, -0.034749999999999996, 0.04745, 0.0033500000000000005, 0.0183, -0.02425], [0.03455, 0.0043, -0.03005, -0.02905, -0.0008, 0.023150000000000004, -0.004200000000000001, -0.0483, -0.023200000000000002, -0.009550000000000001], [-0.03725, -0.02935, -0.04605000000000001, 0.011000000000000001, -0.03725, -0.02185, 0.0485, 0.03, 0.0378, -0.02015], [0.013750000000000002, 0.02935, 0.006350000000000001, -0.025750000000000002, -0.018750000000000003, 0.0235, -0.03515, 0.04825, -0.039700000000000006, 0.00825], [0.0244, -0.00315, -0.0213, 0.025, -0.04580000000000001, -0.0174, 0.02855, 0.04795, 0.028499999999999998, -0.027000000000000003], [0.02375, -0.00905, -0.006450000000000001, 0.0164, 0.008100000000000001, 0.00745, -0.0076, 0.03175, -0.0342, 0.033350000000000005], [0.04530000000000001, -0.0342, 0.03, -0.0257, -0.0469, 0.01035, -0.04580000000000001, -0.01825, -0.0077, -0.023350000000000003], [-0.0122, -0.04105, -0.0177, 0.0228, -0.0221, 0.015550000000000001, 0.04235, -0.012450000000000001, 0.0429, -0.042050000000000004], [-0.0454, -0.02585, -0.019100000000000002, -0.0482, 0.0158, -0.04755, -0.04365, -0.0237, -0.0177, 0.0059], [0.0056500000000000005, 0.036, 0.0374, -0.0032500000000000003, 0.049550000000000004, -0.032650000000000005, 0.020900000000000002, 0.00030000000000000003, -0.03015, 0.039850000000000003], [-0.0031000000000000003, -0.022150000000000003, 0.0072, -0.0052, 0.046200000000000005, 0.026750000000000003, 0.0037, -0.03765, -0.023150000000000004, -0.02975], [0.010700000000000001, -0.0012500000000000002, -0.00205, -0.0436, 0.009600000000000001, -0.0494, 0.0165, 0.013700000000000002, -0.036, 0.016200000000000003], [5e-05, 0.004050000000000001, -0.00445, -0.0183, 0.0079, -0.044000000000000004, -0.0436, 0.0077, 0.03145, 0.041], [0.0211, 0.03605, -0.03415000000000001, 0.026350000000000002, 0.009750000000000002, -0.010050000000000002, 0.0189, -0.01915, -0.0009, -0.01065], [-0.0257, -0.010000000000000002, 0.0318, 0.02155, -0.0227, 0.044050000000000006, -0.0108, -0.0106, -0.04055, 0.03950000000000001], [-0.015700000000000002, 0.019000000000000003, -0.00235, -0.016700000000000003, -0.04875, -0.0482, -0.03145, 0.03755, -0.0174, 0.01745], [0.0128, -0.04445, -0.01625, 0.013700000000000002, -0.0077, -0.0313, 0.0244, 0.03585, 0.03495, 0.034350000000000006], [0.04675000000000001, 0.00225, 0.039700000000000006, 0.006350000000000001, -0.020900000000000002, 0.0149, -0.04815, -0.019250000000000003, -0.03735, 0.01285], [-0.03615, 0.04945, 0.012150000000000001, -0.03175, 0.00805, -0.039700000000000006, -0.01495, -0.00855, -0.04045000000000001, -0.01055], [0.045200000000000004, 0.00795, 0.04365, 0.006700000000000001, 0.020250000000000004, 0.0099, 0.03855, -0.0373, -0.033, 0.032400000000000005], [-0.043300000000000005, -0.013400000000000002, 0.03, 0.0483, -0.026500000000000003, 0.00715, 0.0235, 0.012, -0.033100000000000004, -0.0236], [0.0027, 0.04945, 0.02145, 0.02295, 0.0391, -0.03935, -0.03950000000000001, -0.006850000000000001, -0.041550000000000004, -0.00805], [0.04805, 0.033850000000000005, -0.012750000000000001, -0.04725, -0.0056500000000000005, -0.0479, -0.01035, 0.0318, 0.0007000000000000001, -0.0206], [-0.019950000000000002, 0.020000000000000004, 0.048, 0.009600000000000001, 0.04000000000000001, -0.044950000000000004, 0.019200000000000002, 0.0092, -0.0299, 0.025500000000000002], [-0.046400000000000004, -0.03105, 0.04795, -0.0092, 0.0349, -0.00135, 0.042050000000000004, 0.041550000000000004, 0.026800000000000004, -0.02155], [-0.03305, 0.009750000000000002, 0.027550000000000005, 0.00595, -0.0036, -0.0316, 0.03805, 0.0079, 0.009, -0.0335], [-0.0037, -0.0119, 0.04485, 0.0076, 0.01685, 0.044500000000000005, -0.0415, 0.0043, -0.0073, -0.030850000000000002], [-0.015700000000000002, 0.03305, -0.022000000000000002, -0.00865, 0.045750000000000006, -0.0477, 0.02195, 0.0366, 0.046250000000000006, -0.00055], [-0.0017500000000000003, 0.04675000000000001, 0.04445, 0.027600000000000003, -0.0239, 0.031100000000000003, -0.00805, 0.0267, -0.034050000000000004, 0.0019], [-0.042550000000000004, -0.04005, 0.0073, 0.01055, -0.011550000000000001, 0.02345, -0.0070999999999999995, 0.01765, -0.015050000000000001, -0.007000000000000001], [0.022750000000000003, -0.045700000000000005, 0.043550000000000005, 0.00775, -0.048, -0.0407, 0.04775, -0.005850000000000001, -0.02845, -0.0091], [0.02375, -0.03755, -0.0147, -0.0414, -0.026750000000000003, -0.0024000000000000002, -0.01685, -0.0313, 0.0012500000000000002, 0.016450000000000003], [-0.012, 0.03635, -0.0429, -0.00435, -0.00095, -0.01685, 0.043750000000000004, 0.03545, -0.04530000000000001, 0.027000000000000003], [-0.04605000000000001, -0.0461, 0.0161, -0.0352, -0.0446, 0.044700000000000004, 0.0353, 0.00515, -0.0029000000000000002, 0.0488], [-0.022750000000000003, -0.02045, -0.03515, 0.00705, -0.0489, -0.049800000000000004, 0.04185, -0.01235, -0.00185, 0.009600000000000001], [0.020100000000000003, -0.00495, -0.0143, -0.01785, -0.00285, 0.042800000000000005, 0.03905, 0.03905, -0.011550000000000001, 0.025300000000000003], [0.02125, 0.04895, 0.037200000000000004, -0.027650000000000004, -0.04825, 0.014199999999999999, -0.012150000000000001, 0.046150000000000004, -0.043550000000000005, 0.005350000000000001], [0.006750000000000001, -0.006850000000000001, -0.01675, 0.043800000000000006, 0.009850000000000001, -0.038900000000000004, 0.0097, -0.038700000000000005, 0.03155, 0.0149], [0.0102, 0.04195, 0.02135, -0.0296, -0.006950000000000001, 0.01525, 0.02525, -0.05, 0.0174, -0.0446], [0.018600000000000002, -0.04105, -0.03415000000000001, 0.020200000000000003, -0.005050000000000001, -0.0006000000000000001, 0.0086, 0.033800000000000004, 0.00945, -0.0077], [-0.044250000000000005, -0.00515, 0.026750000000000003, -0.04445, -0.03735, -0.049, 0.0294, 0.037200000000000004, -0.0012000000000000001, -0.04265], [-0.004900000000000001, 0.043000000000000003, 0.0459, 0.043250000000000004, -0.004, 0.0149, -0.0354, -0.0016500000000000002, 0.0237, -0.01455], [-0.005600000000000001, 0.01515, 0.01825, 0.032850000000000004, -0.04050000000000001, -0.008450000000000001, 0.04505000000000001, -0.0454, -0.02425, -0.046200000000000005], [-0.0046, 0.0422, 0.00025, 0.018850000000000002, 0.0354, 0.0184, -0.040350000000000004, 0.03925000000000001, 0.027950000000000003, -0.002]]


class Type(Enum):
    crossing = "crossing"
    high_way = "high_way"
    main_road = "main_road"
    total = "total"


def get_F1(prec, recall):
    return 2 * (prec * recall) / (prec + recall)


class Environment:

    def __init__(self, _type: str):
        super().__init__()
        # self.meta_url = "C:\\Users\\lily\\PycharmProjects\\zhangruoyi\\yolov5\\results2\\metadata"
        self.device = torch.device("cuda:0")
        self.client_num = 10
        self._type = _type
        # self.lvl = [(1, 240), (1, 240), (2, 240), (2, 480), (3, 640), (1, 480), (2, 240), (2, 240), (1, 240), (1, 240)]
        self.lvl = [(2, 640) for i in range(10)]
        img_num = {"crossing": [250, 250, 250, 41, 46, 44, 47, 49, 49, 47],
                   "main_road": [272, 272, 272, 29, 36, 33, 45, 43, 38, 34],
                   "high_way": [243, 243, 243, 29, 48, 37, 41, 32, 44, 40]}
        self.img_num = img_num.get(_type).copy()
        self.latency = [0.2987, 0.4022, 0.3346, 0.3474, 0.2572, 0.2052, 0.3082, 0.1177, 0.2522, 0.2263]
        # self.latency_add = [random.randint(-1000, 1000) / 1000 * 0.05 for i in range(10)]
        self.latency_add = constant_add[0]
        self.gamma = (0.5, 0.5)
        self.delta = 1.5

        opts = [Opt(
            weights='C:\\Users\\lily\\PycharmProjects\\zhangruoyi\\yolov5\\base_models2\\{}\\client{}\\epoch{}.pt'.format(
                self._type, i, self.lvl[i][0] - 1),
            device='0',
            # epoch会实时影响学习率，因此我们应该假定A是多少轮的模型，然后每次只执行其中的若干步， 因此epoch和helper的lvl属性务必认真填
            data='C:\\Users\\lily\\PycharmProjects\\zhangruoyi\\yolov5\\yml\\{}\\client{}.yml'.format(self._type, i),
            epochs=150,
            imgsz=self.lvl[i][1], ) for i in range(10)]

        self.client = [TrainingHelper(opts[i], self.lvl[i][0], 0) for i in
                       range(self.client_num)]

        val_opts = Opt(
            weights='C:\\Users\\lily\\PycharmProjects\\zhangruoyi\\yolov5\\yolov5n.pt',
            device='0',
            # epoch会实时影响学习率，因此我们应该假定A是多少轮的模型，然后每次只执行其中的若干步， 因此epoch和helper的lvl属性务必认真填
            data='C:\\Users\\lily\\PycharmProjects\\zhangruoyi\\yolov5\\yml\\{}\\val.yml'.format(self._type),
            epochs=150,
            imgsz=640, )

        self.val_client = TrainingHelper(val_opts, 3, 0)
        self.epoch = 75
        self.step = 0

    def reset(self):
        opts = [Opt(
            weights='C:\\Users\\lily\\PycharmProjects\\zhangruoyi\\yolov5\\base_models2\\{}\\client{}\\epoch{}.pt'.format(
                self._type, i, self.lvl[i][0] - 1),
            device='0',
            # epoch会实时影响学习率，因此我们应该假定A是多少轮的模型，然后每次只执行其中的若干步， 因此epoch和helper的lvl属性务必认真填
            data='C:\\Users\\lily\\PycharmProjects\\zhangruoyi\\yolov5\\yml\\{}\\client{}.yml'.format(self._type, i),
            epochs=150,
            imgsz=self.lvl[i][1], ) for i in range(10)]

        self.client = [TrainingHelper(opts[i], self.lvl[i][0], 0) for i in
                       range(self.client_num)]

        val_opts = Opt(
            weights='C:\\Users\\lily\\PycharmProjects\\zhangruoyi\\yolov5\\yolov5n.pt',
            device='0',
            # epoch会实时影响学习率，因此我们应该假定A是多少轮的模型，然后每次只执行其中的若干步， 因此epoch和helper的lvl属性务必认真填
            data='C:\\Users\\lily\\PycharmProjects\\zhangruoyi\\yolov5\\yml\\{}\\val.yml'.format(self._type),
            epochs=150,
            imgsz=640, )

        self.val_client = TrainingHelper(val_opts, 3, 0)
        gc.collect()

        self.step = 0
        self.latency_add = constant_add[0]

    def get_state(self):
        states = []
        models = []
        models.extend([self.client[i].model.state_dict() for i in range(self.client_num)])
        pca = get_pca_by_model(models)
        for i in range(0, self.client_num):
            states.extend(pca[i])
        states.extend(self.gamma)
        states.extend([self.latency[i] + self.latency_add[i] for i in range(10)])
        return states

    # 往global中填充增量
    def sum_model(self, actions):
        # 根据train次数为参数赋权
        # 总train次数
        tmp_locals = []
        total_img_num = 0
        # 训练出增量
        for action in actions:
            total_img_num += self.img_num[action]
            tmp_locals.append(self.client[action].model.state_dict())
            self.client[action].model.load_state_dict(self.val_client.model.state_dict())
            self.client[action].train()
        # 带增量的模型, (模型， 权重) 使用平权
        models = [(self.client[actions[i]].model.state_dict(), self.img_num[actions[i]] / total_img_num) for i
                  in range(len(actions))]
        tmp_global = {}
        for key, var in self.val_client.model.state_dict().items():
            new = var.clone()
            # 给local附加增量
            for i in range(len(actions)):
                tmp_local = tmp_locals[i]
                now_local = models[i][0]
                add = now_local.get(key) - var
                # local模型迭代 local的增量 + local的原值
                tmp_local.update({key: add + tmp_local.get(key)})
                # global增量迭代 增量 * 权
                if new.dtype == torch.int64:
                    new += (add * models[i][1]).long()
                else:
                    new += add * models[i][1]
            # global更新 总增量 + 初值
            tmp_global.update({key: new})
        # 还原local
        for pos, action in enumerate(actions):
            self.client[action].model.load_state_dict(tmp_locals[pos])
        self.val_client.model.load_state_dict(tmp_global)

    def get_latency(self, actions):
        tmp = 0
        for action in actions:
            if self.latency[action] + self.latency_add[action] > tmp:
                tmp = self.latency[action] + self.latency_add[action]
        return tmp

    def get_actions(self, action):
        latency = self.latency[action] + self.latency_add[action]
        results = []
        for i in range(10):
            if self.latency[i] + self.latency_add[i] <= latency:
                results.append(i)
        return results

    # 目前定义25个维度 action是從0開始的
    def next(self, actions):
        self.sum_model(actions)
        self.step += 1
        # 3、测试global
        mp, mr, map50, _, _, _, _ = self.val_client.val()
        latency = self.get_latency(actions)
        reward = self.delta * self.gamma[0] * get_F1(mp, mr) - self.gamma[1] * latency
        done = 0
        if self.step >= self.epoch:
            done = 1
        last_latency = [self.latency[i] + self.latency_add[i] for i in range(10)]
        # 更新下一個周期的latency
        if self.step < self.epoch:
            self.latency_add = constant_add[self.step]
        return reward, (mp, mr, map50), done, latency, last_latency


if __name__ == '__main__':
    print([[0.85 + (math.log(math.e + i) / j) for i in range(9)] for j in [3.5, 3, 3.3]])